{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e456fe-7513-425b-b9d3-14727f9db981",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import ast\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from elasticsearch import Elasticsearch\n",
    "import json\n",
    "import urllib3\n",
    "urllib3.disable_warnings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47357f1-2d01-4af4-9cf7-e664565c68de",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('config.json') as f:\n",
    "    data = json.load(f)\n",
    "es_username = data['es_username']\n",
    "es_password = data['es_password']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4378abe6-097e-432b-b2c5-c8b58dbdebce",
   "metadata": {},
   "source": [
    "## Microsoft GraphRAG Ontology\n",
    "Ontologies enable us to define and also validate our Knowledge Graph. They have been around for a while and I've built one for the Knowledge Graph we'll be using. I used a cool product called [Metaphactory](https://metaphacts.com/) to create this and this is what it looks like:\n",
    "\n",
    "![](../images/msft_graphrag_ontology.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40596e1e-f4f1-4fd9-a652-0e2c16d23466",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_folder = 'D:/Data/RDF'\n",
    "graphrag_folder = 'D:/Logs/ragtest/output/20240804-161103/artifacts/'  # 1200 chunk size and covariate records\n",
    "f_ttl = open(os.path.join(output_folder, 'msft-graphrag-1200-chunk-size-plus-covariates.ttl'), 'w', encoding='utf-8')\n",
    "#graphrag_folder = 'D:/Logs/ragtest/output/20240806-182605/artifacts/'  # 300 chunk size and no covariates\n",
    "#f_ttl = open(os.path.join(output_folder, 'msft-graphrag-300-chunk-size.ttl'), 'w', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe493ec-d153-4222-abde-13d19eb2cc8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the header\n",
    "f_ttl.write(\"@prefix gr: <http://ormynet.com/ns/msft-graphrag#> .\\n\")  # graph definition\n",
    "f_ttl.write(\"@prefix d: <http://ormynet.com/ns/data#> .\\n\")            # instance data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c10264-d7d1-49ba-8cf8-1c8a81682b26",
   "metadata": {},
   "source": [
    "## Import Document\n",
    "Import the Document data from the final documents parquet file. We'll add instances of the `Document` class for each record we read there. We'll only need the **id** and **title** fields as the other fields can be obtained through the relationships that will be created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31754f2b-c952-44b1-9c4d-9de01efbc8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_df = pd.read_parquet(os.path.join(graphrag_folder, 'create_final_documents.parquet'))\n",
    "doc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff7e488-fdcb-4461-9535-b9f6090e3a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_document_ttl(df: pd.DataFrame, recnum: int):\n",
    "    \"\"\"Add a Document to the turtle file\n",
    "\n",
    "    :param df: Document dataframe\n",
    "    :param recnum: record number we are looking at\n",
    "    \"\"\"\n",
    "    doc_id = df['id'].iloc[recnum]\n",
    "    title = df['title'].iloc[recnum]\n",
    "    f_ttl.write(f'\\nd:Document_{doc_id} a gr:Document ;\\n')\n",
    "    f_ttl.write(f'    gr:id \"{doc_id}\";\\n')\n",
    "    f_ttl.write(f'    gr:title \"{title}\" .\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5f9bb3-90c8-4778-a1ae-62c79c458ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm(range(0, len(doc_df))):\n",
    "    write_document_ttl(doc_df, i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "babc2d33-fa68-4c83-876c-fc87293a0e9b",
   "metadata": {},
   "source": [
    "## Import Text Units\n",
    "Import the text units data from the final text units parquet file. We'll add instances of the `Chunk` class for each record we read. We'll also link each `Chunk` back to it's `Document` using the `part_of` relationship."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cdb37dd-0fd0-42c6-9668-316f2a212754",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_df = pd.read_parquet(os.path.join(graphrag_folder, 'create_final_text_units.parquet'))\n",
    "text_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ebabcd-b5a4-4faf-9ef9-b5a02b8edd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_chunk_with_ttl(df: pd.DataFrame, recnum: int):\n",
    "    \"\"\"Add a Chunk to the turtle file\n",
    "\n",
    "    :param df: text units dataframe\n",
    "    :param recnum: record number we are looking at\n",
    "    \"\"\"\n",
    "    chunk_id = df['id'].iloc[recnum]\n",
    "    text = df['text'].iloc[recnum]\n",
    "    n_tokens = df['n_tokens'].iloc[recnum]\n",
    "    f_ttl.write(f'\\nd:Chunk_{chunk_id} a gr:Chunk ;\\n')\n",
    "    f_ttl.write(f'    gr:id \"{chunk_id}\";\\n')\n",
    "    if text is not None:\n",
    "        f_ttl.write('    gr:text \"\"\"\\n')\n",
    "        f_ttl.write(text)\n",
    "        f_ttl.write('\\n\"\"\";\\n')\n",
    "    f_ttl.write(f'    gr:n_tokens {n_tokens} .\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea913934-d0cf-4e28-ab98-a555f5ecb245",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_chunk_part_of_with_ttl(chunk_id: str, document_id: str):\n",
    "    \"\"\"Add 'part_of' link to the turtle file\n",
    "\n",
    "    :param chunk_id: id of the Chunk\n",
    "    :param document_id: id of the Document\n",
    "    \"\"\"\n",
    "    f_ttl.write(f'\\nd:Chunk_{chunk_id} gr:part_of d:Document_{document_id} .\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457bf233-b316-48c1-b2df-5381693cfa80",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm(range(0, len(text_df))):\n",
    "    add_chunk_with_ttl(text_df, i)\n",
    "    chunk_id = text_df['id'].iloc[i]\n",
    "    document_ids = text_df['document_ids'].iloc[i].tolist()\n",
    "    for document_id in document_ids:\n",
    "        add_chunk_part_of_with_ttl(chunk_id, document_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e02509e-3a14-420f-86f8-fa9ab0bfb985",
   "metadata": {},
   "source": [
    "## Import Entities\n",
    "Import the entities data from the final entities parquet file. We'll add instances of the `Entity` class for each record we read. We'll also link each `Chunk` instance that contains this `Entity` using the `has_entity` relationship."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742789e6-7045-4361-84dc-6fc65f69b578",
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_df = pd.read_parquet(os.path.join(graphrag_folder, 'create_final_entities.parquet'))\n",
    "entity_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5186d598",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_leading_trailing_double_quotes(x) -> str:\n",
    "    \"\"\"Remove double-quotes from a string only if they are at the start and end of a string\n",
    "\n",
    "    :param x: string to be parsed\n",
    "    :returns: trimmed string\n",
    "    \"\"\"\n",
    "    trimmed_string = x\n",
    "    if len(x) > 1:  # ensure the string is long enough to have 2 double-quotes\n",
    "        if x[0] =='\"' and x[-1] == '\"':\n",
    "            trimmed_string = x[1:-1]\n",
    "    return trimmed_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a8a221-927e-480e-8ee9-24dcf6abea86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_has_entity_with_ttl(chunk_id: str, entity_id: str):\n",
    "    \"\"\"Add has_entity relationship to the turtle file\n",
    "\n",
    "    :param chunk_id: id of the Chunk\n",
    "    :param entity_id: id of the Entity\n",
    "    \"\"\"\n",
    "    f_ttl.write(f'\\nd:Chunk_{chunk_id}  gr:has_entity d:Entity_{entity_id} . \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c39907-9c2d-4955-871a-ba4612e447cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_entity_with_ttl(df: pd.DataFrame, recnum: int):\n",
    "    \"\"\"Add an Entity instance to the turtle file\n",
    "    \n",
    "    :param df: datafram containing the Entity data\n",
    "    :param recnum: record number we are looking at\n",
    "    \"\"\"\n",
    "    entity_id = df['id'].iloc[recnum]\n",
    "    name = df['name'].iloc[recnum]\n",
    "    entity_type = df['type'].iloc[recnum]\n",
    "    description = df['description'].iloc[recnum]\n",
    "    human_readable_id = df['human_readable_id'].iloc[recnum]\n",
    "    f_ttl.write(f'\\nd:Entity_{entity_id} a gr:Entity;\\n')\n",
    "    f_ttl.write(f'    gr:id \"{entity_id}\";\\n')\n",
    "    if name is not None:\n",
    "        name = remove_leading_trailing_double_quotes(name)\n",
    "        f_ttl.write(f'    gr:name \"{name}\";\\n')\n",
    "    if entity_type is not None:\n",
    "        entity_type = remove_leading_trailing_double_quotes(entity_type)\n",
    "        f_ttl.write(f'    gr:type \"{entity_type}\";\\n')\n",
    "    if description is not None:\n",
    "        f_ttl.write('    gr:description \"\"\"\\n')\n",
    "        f_ttl.write(remove_leading_trailing_double_quotes(description))\n",
    "        f_ttl.write('\\n\"\"\";\\n')\n",
    "    f_ttl.write(f'    gr:human_readable_id \"{human_readable_id}\" .\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35fa7ed2-7807-4f4a-ac70-6ed1c19feed1",
   "metadata": {},
   "source": [
    "Add all the `Entity` instances. We also create an **entity_lookup** table that we will use when setting up relationships later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449bf235-76d7-4a6e-b448-6f640e847b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_lookup = {}  # this will become our lookup table when creating relationships later on\n",
    "for i in tqdm(range(0, len(entity_df))):\n",
    "    add_entity_with_ttl(entity_df, i)\n",
    "    entity_id = entity_df['id'].iloc[i]\n",
    "    name = remove_leading_trailing_double_quotes(entity_df['name'].iloc[i])\n",
    "    entity_lookup[name] = entity_id  # store this for later relationships\n",
    "    text_unit_ids = entity_df['text_unit_ids'].iloc[i].tolist()\n",
    "    for text_unit_id in text_unit_ids:\n",
    "        add_has_entity_with_ttl(text_unit_id, entity_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab98009-d5aa-4b5e-ba49-06f684e16a1b",
   "metadata": {},
   "source": [
    "## Import Relationships\n",
    "Import the relationships data from the final relationships parquet file. For the relationships we use the **entity_lookup** table we created to find the actual **id** of the source and target `Entity`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faefa322-a5d3-4724-9663-75f95e883f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_df = pd.read_parquet(os.path.join(graphrag_folder, 'create_final_relationships.parquet'))\n",
    "rel_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9ecce7-f859-4c39-9359-94e449930b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_relationship_with_ttl(df: pd.DataFrame, recnum: int):\n",
    "    \"\"\"Add related_to to the turtle file\n",
    "\n",
    "    :param df: dataframe containing the relationship data\n",
    "    :param recnum: record number we are looking at\n",
    "    \"\"\"\n",
    "    source_id = entity_lookup[remove_leading_trailing_double_quotes(df['source'].iloc[recnum])]\n",
    "    target_id = entity_lookup[remove_leading_trailing_double_quotes(df['target'].iloc[recnum])]\n",
    "    rel_id = df['id'].iloc[recnum]\n",
    "    rank = df['rank'].iloc[recnum]\n",
    "    weight = df['weight'].iloc[recnum]\n",
    "    description = remove_leading_trailing_double_quotes(df['description'].iloc[recnum])\n",
    "    text_unit_ids = df['text_unit_ids']\n",
    "    human_readable_id = df['human_readable_id'].iloc[recnum]\n",
    "    relationships_lookup[rel_id] = (source_id, target_id)\n",
    "    f_ttl.write(f'\\nd:related_to_{rel_id} a gr:related_to;\\n')\n",
    "    f_ttl.write(f'    gr:id \"{rel_id}\";\\n')\n",
    "    f_ttl.write(f'    gr:rank {rank};\\n')\n",
    "    f_ttl.write(f'    gr:weight {weight};\\n')\n",
    "    if description is not None:\n",
    "        f_ttl.write('    gr:description \"\"\"\\n')\n",
    "        f_ttl.write(description)\n",
    "        f_ttl.write('\\n\"\"\";\\n')\n",
    "    f_ttl.write(f'    gr:human_readable_id \"{human_readable_id}\" .\\n')\n",
    "    f_ttl.write(f'\\nd:Entity_{source_id} d:related_to_{rel_id} d:Entity_{target_id} .\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a6a2e2-517c-4027-b76a-28a4376ba19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "relationships_lookup = {}\n",
    "for i in tqdm(range(0, len(rel_df))):\n",
    "    add_relationship_with_ttl(rel_df, i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c48c83a-db72-4e76-b068-5f961c3ff743",
   "metadata": {},
   "source": [
    "## Import Community Data\n",
    "Import the community data from the final communities parquet file. We'll add instances of the `Community` class for each record we read. We'll also Add the community relationships of `related_to` and `in_community`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517aef58-1191-4f01-9437-04102e987d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "community_df = pd.read_parquet(os.path.join(graphrag_folder, 'create_final_communities.parquet'))\n",
    "community_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66a37dc-6501-48f1-9e4b-4df775c00703",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_community_with_ttl(df: pd.DataFrame, recnum: int):\n",
    "    \"\"\"Add a Community instance to the turtle file\n",
    "\n",
    "    :param df: dataframe containing the community records\n",
    "    :param recnum: record number we are looking at\n",
    "    \"\"\"\n",
    "    community_id = df['id'].iloc[recnum]\n",
    "    level = df['level'].iloc[recnum]\n",
    "    f_ttl.write(f'\\nd:Community_{community_id} a gr:Community;\\n')\n",
    "    f_ttl.write(f'    gr:id \"{community_id}\";\\n')\n",
    "    f_ttl.write(f'    gr:level {level} .\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5284c463-079a-41bc-83ec-72f18305b13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_community_relationships_with_ttl(community_id:str, source_id: str, target_id: str, related_id: str):\n",
    "    \"\"\"Add Comunity relationships to the turtle file\n",
    "\n",
    "    :param community_id: id of the Community instance\n",
    "    :param source_id: id of the source Entity instance\n",
    "    :param target_id: id of the target Entity instance\n",
    "    :param related_id: id of the related_to instance\n",
    "    \"\"\"\n",
    "    f_ttl.write(f'\\nd:related_to_{related_id} a gr:related_to .\\n')\n",
    "    f_ttl.write(f'\\nd:Entity_{source_id} d:related_to_{related_id} d:Entity_{target_id} .\\n')\n",
    "    f_ttl.write(f'\\nd:Entity_{source_id} gr:in_community d:Community_{community_id} .\\n')\n",
    "    f_ttl.write(f'\\nd:Entity_{target_id} gr:in_community d:Community_{community_id} .\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ebf579d-fe4d-4945-926f-6d24b0acab28",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm(range(0, len(community_df))):\n",
    "    add_community_with_ttl(community_df, i)\n",
    "    community_id = community_df['id'].iloc[i]\n",
    "    relationship_ids = community_df['relationship_ids'].iloc[i].tolist()\n",
    "    for relationship_id in relationship_ids:\n",
    "        source_id, target_id = relationships_lookup[relationship_id]\n",
    "        add_community_relationships_with_ttl(community_id, source_id, target_id, relationship_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130f7edb-13c7-4340-8429-e377a1fb2738",
   "metadata": {},
   "source": [
    "## Import Community Reports\n",
    "Import the community reports data from the final community reports parquet file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e047d48-d622-418b-b386-708a0264804d",
   "metadata": {},
   "outputs": [],
   "source": [
    "community_report_df = pd.read_parquet(os.path.join(graphrag_folder, 'create_final_community_reports.parquet'))\n",
    "community_report_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53517be-94ec-45b6-a257-0828d9fa3fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_community_report_with_ttl(df: pd.DataFrame, recnum: int):\n",
    "    \"\"\"Add the Community report with turtle\n",
    "\n",
    "    :param df: dataframe containing the community report records\n",
    "    :param recnum: record number we are looking at\n",
    "    \"\"\"\n",
    "    community_id = df['community'].iloc[recnum]\n",
    "    findings_id = df['id'].iloc[recnum]\n",
    "    level = df['level'].iloc[recnum]\n",
    "    title = df['title'].iloc[recnum]\n",
    "    summary = df['summary'].iloc[recnum]\n",
    "    findings = df['findings'].iloc[recnum]\n",
    "    rank = df['rank'].iloc[recnum]\n",
    "    rank_explanation = df['rank_explanation'].iloc[recnum]\n",
    "    full_content = df['full_content'].iloc[recnum]\n",
    "    f_ttl.write(f'\\nd:Community_{community_id} gr:rank \"{rank}\";\\n')\n",
    "    f_ttl.write(f'    gr:level {level};\\n')\n",
    "    f_ttl.write(f'    gr:title \"{title}\";\\n')\n",
    "    f_ttl.write('    gr:rank_explanation \"\"\"\\n')\n",
    "    f_ttl.write(f'{rank_explanation}')\n",
    "    f_ttl.write('\\n\"\"\";\\n')\n",
    "    f_ttl.write('    gr:full_content \"\"\"\\n')\n",
    "    f_ttl.write(f'{full_content}')\n",
    "    f_ttl.write('\\n\"\"\";\\n')\n",
    "    f_ttl.write('    gr:summary \"\"\"\\n')\n",
    "    f_ttl.write(f'{summary}')\n",
    "    f_ttl.write('\\n\"\"\" .\\n')\n",
    "    # Findings\n",
    "    finding_id_start = findings_id.replace('-', '')\n",
    "    for i, f in enumerate(findings):\n",
    "        f_ttl.write(f'\\nd:Finding_{finding_id_start}_{i} a gr:Finding;\\n')\n",
    "        f_ttl.write('    gr:finding \"\"\"\\n')\n",
    "        f_ttl.write(f'{f}')\n",
    "        f_ttl.write('\\n\"\"\" .\\n')\n",
    "        f_ttl.write(f'\\nd:Community_{community_id} gr:has_finding d:Finding_{finding_id_start}_{i} .\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d52825-dd83-4fff-a7b7-957c47a461c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm(range(0, len(community_report_df))):\n",
    "    add_community_report_with_ttl(community_report_df, i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc7d72ee-0c73-4a91-aad1-a9a6e03438cc",
   "metadata": {},
   "source": [
    "## Import Covariates\n",
    "Read all the covariates from the final covariates parquet file. This file will only have been created if you set the flag to extract claims in the config file. I set it once and it added about a day to my run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1504f8a-bd4e-477b-a668-b59da8cc0c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "covariate_df = pd.read_parquet(os.path.join(graphrag_folder, 'create_final_covariates.parquet'))\n",
    "covariate_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5bae66-8ea1-4cf5-8ccb-e73b725063f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_covariate_with_ttl(df: pd.DataFrame, recnum: int):\n",
    "    \"\"\"Add the Covariate with turtle\n",
    "\n",
    "    :param df: dataframe containing the covariate records\n",
    "    :param recnum: record number we are looking at\n",
    "    \"\"\"\n",
    "    covariate_id = df['id'].iloc[recnum]\n",
    "    text_unit_id = df['text_unit_id'].iloc[recnum]\n",
    "    document_ids = df['document_ids'].iloc[recnum]\n",
    "    n_tokens = df['n_tokens'].iloc[recnum]\n",
    "    f_ttl.write(f'\\nd:Covariate_{covariate_id} a gr:Covariate .\\n')\n",
    "    if document_ids is not None:\n",
    "        f_ttl.write(f'd:Covariate_{covariate_id} gr:document_ids \"{document_ids}\" .\\n')\n",
    "    if n_tokens is not None:\n",
    "        f_ttl.write(f'd:Covariate_{covariate_id} gr:n_tokens \"{n_tokens}\" .\\n')\n",
    "    if text_unit_id is not None:\n",
    "        f_ttl.write(f'd:Covariate_{covariate_id} gr:text_unit_id \"{text_unit_id}\" .\\n')\n",
    "        f_ttl.write(f'\\nd:Entity_{text_unit_id} gr:has_covariate \"{covariate_id}\" .\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26678c61-492f-4f69-bedc-82ee95225a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm(range(0, len(covariate_df))):\n",
    "    add_covariate_with_ttl(covariate_df, i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0353a946-4ef8-4c51-a7b8-b61be3639331",
   "metadata": {},
   "source": [
    "Close our turtle file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd9eefe-e8bd-4190-a1e0-23604e3cae17",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_ttl.flush()\n",
    "f_ttl.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f744f5e-de4d-4552-9921-9af5afbcc872",
   "metadata": {},
   "source": [
    "## Create Elasticsearch Index\n",
    "Create an Elasticsearch index using the `Entity` **description_embedding** data. In order to do this part your Elasticsearch server must be up and running. I've got mine running on my local machine. When it first starts it generates a random password for the server. Put this into the config.json file in this folder. The username is always `elastic`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67cc5ce6-5cb6-416d-a21c-9807362da656",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_vector = entity_df['description_embedding'].iloc[0]\n",
    "print(f'description_embedding - dimensions: {len(embedding_vector)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b27033-e326-4e8e-8f55-bbf9d9d978ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "es = Elasticsearch(\"https://localhost:9200\", \n",
    "                   basic_auth=(es_username, es_password), \n",
    "                   verify_certs=False)\n",
    "es.info().body"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39dfcae4-7f97-4e4c-9056-248777ecf1e5",
   "metadata": {},
   "source": [
    "We're going to configure the index mappings. We tell it the name of the column we'll be using as our index (in our case it's **description_embedding**) and then we add all the other columns we'd like to return with our search. We only need the **id** field. That will be enough to lookup our corresponding `Entity` record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f45a8c7-2163-4ef8-8e5d-523b366d0650",
   "metadata": {},
   "outputs": [],
   "source": [
    "indexMapping = {\n",
    "    \"properties\":{\n",
    "        \"id\":{\n",
    "            \"type\":\"keyword\"\n",
    "        },\n",
    "        \"description_embedding\":{\n",
    "            \"type\": \"dense_vector\",\n",
    "            \"dims\": len(embedding_vector),\n",
    "            \"index\": True,\n",
    "            \"similarity\": \"l2_norm\",\n",
    "        }, \n",
    "    }\n",
    "}\n",
    "\n",
    "index_name = \"entity_graph_index\" \n",
    "#es.indices.delete(index=index_name)\n",
    "es.indices.create(index=index_name, mappings=indexMapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a663dea3-815a-4cdf-b7b7-4532012b4d00",
   "metadata": {},
   "source": [
    "We need to convert our DataFrame to a dictionary as this is the format Elasticsearch expects when we're adding a record to the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79312336-c996-4e03-b6e7-f3f1e4084792",
   "metadata": {},
   "outputs": [],
   "source": [
    "record_list = entity_df[['id', 'description_embedding']].to_dict(\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f086d0-9733-415e-b5fc-cb862c18fe18",
   "metadata": {},
   "outputs": [],
   "source": [
    "for record in record_list:\n",
    "    es.index(index=index_name, document=record, id=record[\"id\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387b2c90-b307-403b-ab98-d281bb40d425",
   "metadata": {},
   "source": [
    "Verify that all our `Entity` records have been index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03003865-6142-4cef-b245-5361582e8caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_count = es.count(index=index_name)['count']\n",
    "if index_count == len(entity_df):\n",
    "    print(\"Success! All the Entity records have been indexed\")\n",
    "else:\n",
    "    print(\"**ERROR** Failed to index all the records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80305b0f-3baa-4add-b09e-83b686ea4e09",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
